{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_jsonl_for_spacy(path_to_jsonl):\n",
    "    \n",
    "    # open and read the jsonl-file\n",
    "    jsonl_file = open(path_to_jsonl, \"r\")\n",
    "    \n",
    "    lines = jsonl_file.readlines()\n",
    "    jsonl_list = []\n",
    "    \n",
    "    # split the lines of the jsonl-file into text and label\n",
    "    for line in lines:\n",
    "        line = json.loads(line)\n",
    "        if \"label\" in line:\n",
    "            line[\"entities\"] = line.pop(\"label\")\n",
    "        else:\n",
    "            line[\"entities\"] = []\n",
    "\n",
    "        tmp_ents = []\n",
    "        for e in line[\"entities\"]:\n",
    "            tmp_ents.append(e)\n",
    "            \n",
    "        jsonl_list.append((line[\"data\"],{\"entities\" : tmp_ents}))\n",
    "    \n",
    "    # split the data into training and test data\n",
    "    \n",
    "    amount_train_data = round(len(jsonl_list)*0.7)\n",
    "    train_data = jsonl_list[:amount_train_data]\n",
    "    test_data = jsonl_list[amount_train_data:]\n",
    "\n",
    "    return test_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8513ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ner_model(test_data,train_data):\n",
    "    \n",
    "    # load spaCy model\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # load the Named Entity Recognizer of the model and add new labels\n",
    "\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    for i in train_data:\n",
    "        for j in i[1][\"entities\"]:\n",
    "            ner.add_label(j[2])\n",
    "\n",
    "    \n",
    "    nlp.disable_pipes()\n",
    "\n",
    "    infixes = list(nlp.Defaults.infixes)\n",
    "    \n",
    "    # add - as new split object to the tokenizer\n",
    "    \n",
    "    infixes.extend((\"-\"))\n",
    "    infix_regex = spacy.util.compile_infix_regex(infixes)\n",
    "    nlp.tokenizer.infix_finditer = infix_regex.finditer\n",
    "\n",
    "    # define the pipelines that shall be updated\n",
    "    \n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    optimizer = nlp.resume_training()\n",
    "    with nlp.disable_pipes(*unaffected_pipes):\n",
    "    \n",
    "        # Training with 50 iterations\n",
    "        for iteration in tqdm(range(70), desc=\"Iterations\"):\n",
    "\n",
    "            # shuffling examples before every iteration\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                example = []\n",
    "                for i in range(len(texts)):\n",
    "\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "\n",
    "                    \n",
    "                        \n",
    "                nlp.update(\n",
    "                    example,\n",
    "                    sgd=optimizer,\n",
    "                    drop=0.3,  # dropout\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    \n",
    "    output_dir = Path('/path/to/store/model')\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, train_data = transform_jsonl_for_spacy(\"/path/to/jsonlfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ner_model(test_data, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('/path/to/model')\n",
    "\n",
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Foxes infected by rabies have a mortility rate of 100% if not vaccined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
